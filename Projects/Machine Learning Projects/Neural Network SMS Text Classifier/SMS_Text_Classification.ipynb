{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RZOuS9LWQvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "430fb296-f794-48ec-84fb-1c672c720d46"
      },
      "source": [
        "# Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMHwYXHXCar3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87430d47-e093-4e2b-d198-6d876dd7e816"
      },
      "source": [
        "# Getting the data files\n",
        "!wget \"https://raw.githubusercontent.com/beaucarnes/fcc_python_curriculum/master/sms/train-data.tsv\"\n",
        "!wget \"https://raw.githubusercontent.com/beaucarnes/fcc_python_curriculum/master/sms/valid-data.tsv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-23 20:59:58--  https://raw.githubusercontent.com/beaucarnes/fcc_python_curriculum/master/sms/train-data.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358233 (350K) [text/plain]\n",
            "Saving to: ‘train-data.tsv’\n",
            "\n",
            "\rtrain-data.tsv        0%[                    ]       0  --.-KB/s               \rtrain-data.tsv      100%[===================>] 349.84K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2020-11-23 20:59:58 (49.9 MB/s) - ‘train-data.tsv’ saved [358233/358233]\n",
            "\n",
            "--2020-11-23 20:59:59--  https://raw.githubusercontent.com/beaucarnes/fcc_python_curriculum/master/sms/valid-data.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118774 (116K) [text/plain]\n",
            "Saving to: ‘valid-data.tsv’\n",
            "\n",
            "valid-data.tsv      100%[===================>] 115.99K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2020-11-23 20:59:59 (41.1 MB/s) - ‘valid-data.tsv’ saved [118774/118774]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_h508FEClxO"
      },
      "source": [
        "# Importing tsv data to pd dataframes\n",
        "def csv_to_df():\n",
        "  df_train = pd.read_csv(\"train-data.tsv\", sep = \"\\t\", header = None, names = ['class', 'message'])\n",
        "  df_test = pd.read_csv(\"valid-data.tsv\", sep = \"\\t\", header = None, names = ['class','message'])\n",
        "  return df_train, df_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOMKywn4zReN"
      },
      "source": [
        "# Cleaning the data\n",
        "def clean_data(df_train, df_test):\n",
        "  # Handling categorical values\n",
        "  class_train = df_train['class'].astype('category').cat.codes\n",
        "  class_test = df_test['class'].astype('category').cat.codes\n",
        "  return class_train, class_test"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPADMShAET0M"
      },
      "source": [
        "# Cleaning the messages\n",
        "def clean_text(msg):\n",
        "  # Text preprocessing\n",
        "  english_words = set(stopwords.words('english'))\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  msg = re.sub(r'([^\\s\\w])+', ' ', msg)\n",
        "  msg = \" \".join([lemmatizer.lemmatize(words) for words in msg.split() if not words in english_words])\n",
        "  msg = msg.lower()\n",
        "  return msg"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIpcLgOG4ash"
      },
      "source": [
        "# Preprocessing data\n",
        "def preprocessing_data(msg_series):\n",
        "  # Vectorization of data\n",
        "  seq = tknzr.texts_to_sequences(msg_series)\n",
        "  # Padding the data\n",
        "  seq_matrix = sequence.pad_sequences(seq, maxlen = 500)\n",
        "  return seq_matrix"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziW93fRk57o8"
      },
      "source": [
        "# Building the model \n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([tf.keras.layers.Input(shape=[500]),tf.keras.layers.Embedding(1000, 50, input_length=500), tf.keras.layers.LSTM(64),tf.keras.layers.Dense(256, activation='relu'), tf.keras.layers.Dropout(0.5), tf.keras.layers.Dense(1, activation='relu') ])\n",
        "  model.compile(loss='binary_crossentropy',optimizer='RMSprop',metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5glCRRh67N5"
      },
      "source": [
        "# Training the model\n",
        "def train_model(model, seq_matrix_train, class_train):\n",
        "  # Monitoring validation loss to keep it at a minimum and stop training the model if it reaches a minimum\n",
        "  model.fit(seq_matrix_train, class_train, batch_size=128, epochs=10, validation_split=0.2, callbacks= [tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001)] , verbose = 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEKP1uTa7_oF"
      },
      "source": [
        "# Evaluating the model\n",
        "def evaluate_model(model, seq_matrix_test, class_test):\n",
        "  eval = model.evaluate(seq_matrix_test, class_test)\n",
        "  print('Loss: {:.3f}, Accuracy: {:.3f}'.format(eval[0], eval[1]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9tD9yACG6M9"
      },
      "source": [
        "# Predict messages based on model\n",
        "def predict_message(model, pred_text):\n",
        "  pred_text = clean_text(pred_text)\n",
        "  p = model.predict(preprocessing_data(pd.Series([pred_text])))[0]\n",
        "  if p<0.5:\n",
        "    class_label = \"ham\"\n",
        "  else:\n",
        "    class_label = \"spam\" \n",
        "\n",
        "  return (p[0], class_label)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxotov85SjsC"
      },
      "source": [
        "# Testing the model\n",
        "def test_predictions(model):\n",
        "  test_messages = [\"how are you doing today\",\n",
        "                   \"sale today! to stop texts call 98912460324\",\n",
        "                   \"i dont want to go. can we try it a different day? available sat\",\n",
        "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
        "                   \"you have won £1000 cash! call to claim your prize.\",\n",
        "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
        "                   \"wow, is your arm alright. that happened to me one time too\"\n",
        "                  ]\n",
        "\n",
        "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
        "  passed = True\n",
        "\n",
        "  for msg, ans in zip(test_messages, test_answers):\n",
        "    prediction = predict_message(model, msg)\n",
        "    if prediction[1] != ans:\n",
        "      passed = False\n",
        "\n",
        "  if passed:\n",
        "    print(\"You passed the challenge. Great job!\")\n",
        "  else:\n",
        "    print(\"You haven't passed yet. Keep trying.\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQGEWnmaO4ja",
        "outputId": "5ad7315d-2d0e-40f3-c09f-b2a9497ee134"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "  df_train, df_test = csv_to_df()\n",
        "  class_train, class_test = clean_data(df_train, df_test)\n",
        "  msg_train = df_train['message'].apply(lambda message : clean_text(message))\n",
        "  \n",
        "  # Tokenizer fits on cleaned message training data and keeps 1000 frequently occuring words\n",
        "  tknzr = Tokenizer(num_words = 1000)\n",
        "  tknzr.fit_on_texts(msg_train)\n",
        "\n",
        "  seq_matrix_train = preprocessing_data(msg_train)\n",
        "  msg_test = df_test['message'].apply(lambda message : clean_text(message))\n",
        "  seq_matrix_test = preprocessing_data(msg_test)\n",
        "  model = create_model()\n",
        "  print(\"Training the model....\")\n",
        "  train_model(model, seq_matrix_train, class_train)\n",
        "  print(\"Evaluating the model....\")\n",
        "  evaluate_model(model, seq_matrix_test, class_test)\n",
        "  print(\"Testing the model....\")\n",
        "  pred_text = \"how are you doing today?\"\n",
        "  prediction = predict_message(model, pred_text)\n",
        "  print(\"Prediction of text {} is : \".format(pred_text),prediction)\n",
        "  predict_message(model, pred_text)\n",
        "  test_predictions(model)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 50)           50000     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                29440     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 96,337\n",
            "Trainable params: 96,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Training the model....\n",
            "Epoch 1/10\n",
            "27/27 [==============================] - 1s 52ms/step - loss: 0.3567 - accuracy: 0.8762 - val_loss: 0.1938 - val_accuracy: 0.9330\n",
            "Epoch 2/10\n",
            "27/27 [==============================] - 1s 30ms/step - loss: 0.0934 - accuracy: 0.9749 - val_loss: 0.1201 - val_accuracy: 0.9821\n",
            "Epoch 3/10\n",
            "27/27 [==============================] - 1s 34ms/step - loss: 0.0633 - accuracy: 0.9886 - val_loss: 0.0957 - val_accuracy: 0.9868\n",
            "Epoch 4/10\n",
            "27/27 [==============================] - 1s 30ms/step - loss: 0.0624 - accuracy: 0.9919 - val_loss: 0.1097 - val_accuracy: 0.9880\n",
            "Evaluating the model....\n",
            "44/44 [==============================] - 0s 11ms/step - loss: 0.1139 - accuracy: 0.9820\n",
            "Loss: 0.114, Accuracy: 0.982\n",
            "Testing the model....\n",
            "Prediction of text how are you doing today? is :  (0.0, 'ham')\n",
            "You passed the challenge. Great job!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}